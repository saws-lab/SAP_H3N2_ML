{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee069ea",
   "metadata": {},
   "source": [
    "# Model utilities\n",
    "It includes self defined functions for used models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb02fb8",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d1480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Add, BatchNormalization, ReLU, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# for reproduciblility, fix the randomly generated numbers\n",
    "SEED = 100\n",
    "tf.keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c5c13",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "AdaBoost with default hyper-parameters\n",
    "\n",
    "> **Parameters**\n",
    "> - X_train (numpy array): input features to train the model\n",
    "> - y_train (numpy array): output labels for supervised learning of the model\n",
    "> - X_test (numpy array): input features to test the model\n",
    "> - y_test: dummy, not used, default=None\n",
    "\n",
    "> **Returns**\n",
    "> - results (dict): dictionary including:\n",
    ">    - pred_train (numpy array): predictions for training dataset\n",
    ">    - pred_test (numpy array): predictions for test dataset\n",
    ">    - model (object): trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a55fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_baseline(X_train, y_train, X_test, y_test=None):\n",
    "    \n",
    "    '''\n",
    "    Model\n",
    "    '''\n",
    "    model = AdaBoostRegressor(random_state = SEED)\n",
    "    \n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    time_start = time()\n",
    "    model.fit(X_train, y_train)\n",
    "    time_end = time()\n",
    "    print(f\"Time for training: {time_end - time_start}\")\n",
    "    \n",
    "    '''\n",
    "    Testing\n",
    "    '''\n",
    "    results = {}\n",
    "    results['pred_train'] = model.predict(X_train)\n",
    "    results['pred_test']  = model.predict(X_test)\n",
    "    results['model']      = model\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b4c614",
   "metadata": {},
   "source": [
    "## Optimized AdaBoost\n",
    "AdaBoost regressor with optimized hyper-parameters for its top mutation matrix GIAG010101\n",
    "\n",
    "> **Parameters**\n",
    "> - X_train (numpy array): input features to train the model\n",
    "> - y_train (numpy array): output labels for supervised learning of the model\n",
    "> - X_test (numpy array): input features to test the model\n",
    "> - y_test: dummy, not used, default=None\n",
    "\n",
    "> **Returns**\n",
    "> - results (dict): dictionary including:\n",
    ">    - pred_train (numpy array): predictions for training dataset\n",
    ">    - pred_test (numpy array): predictions for test dataset\n",
    ">    - model (object): trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_AdaBoost(X_train, y_train, X_test, y_test=None):\n",
    "    \n",
    "    '''\n",
    "    Model\n",
    "    '''\n",
    "    model = AdaBoostRegressor(DecisionTreeRegressor(max_depth=1860, max_features=0.393686389369039),\n",
    "                              n_estimators=230,\n",
    "                              learning_rate=1.39248292746222,\n",
    "                              random_state=SEED)\n",
    "    \n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    time_start = time()\n",
    "    model.fit(X_train, y_train)\n",
    "    time_end = time()\n",
    "    print(f\"Time for training: {time_end - time_start}\")\n",
    "    \n",
    "    '''\n",
    "    Testing\n",
    "    '''\n",
    "    results = {}\n",
    "    results['pred_train'] = model.predict(X_train)\n",
    "    results['pred_test']  = model.predict(X_test)\n",
    "    results['model']      = model\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d3c39",
   "metadata": {},
   "source": [
    "## Optimized AdaBoost for binary\n",
    "AdaBoost regressor with optimized hyper-parameters for binary encoding. This is used for NextFlu matched parameters simulation.\n",
    "\n",
    "> **Parameters**\n",
    "> - X_train (numpy array): input features to train the model\n",
    "> - y_train (numpy array): output labels for supervised learning of the model\n",
    "> - X_test (numpy array): input features to test the model\n",
    "> - y_test: dummy, not used, default=None\n",
    "\n",
    "> **Returns**\n",
    "> - results (dict): dictionary including:\n",
    ">    - pred_train (numpy array): predictions for training dataset\n",
    ">    - pred_test (numpy array): predictions for test dataset\n",
    ">    - model (object): trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4890976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_AdaBoost_binary(X_train, y_train, X_test, y_test=None):\n",
    "    \n",
    "    '''\n",
    "    Model\n",
    "    '''\n",
    "    model = AdaBoostRegressor(DecisionTreeRegressor(max_depth=7040, max_features=0.419171992638116),\n",
    "                              n_estimators=410,\n",
    "                              learning_rate=1.26852534318595,\n",
    "                              random_state=SEED)\n",
    "    \n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    time_start = time()\n",
    "    model.fit(X_train, y_train)\n",
    "    time_end = time()\n",
    "    print(f\"Time for training: {time_end - time_start}\")\n",
    "    \n",
    "    '''\n",
    "    Testing\n",
    "    '''\n",
    "    results = {}\n",
    "    results['pred_train'] = model.predict(X_train)\n",
    "    results['pred_test']  = model.predict(X_test)\n",
    "    results['model']      = model\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0333720",
   "metadata": {},
   "source": [
    "## Optimized RF model\n",
    "RF model with optimized hyper-parameters for its top mutation matrix AZAE970101\n",
    "\n",
    "> **Parameters**\n",
    "> - X_train (numpy array): input features to train the model\n",
    "> - y_train (numpy array): output labels for supervised learning of the model\n",
    "> - X_test (numpy array): input features to test the model\n",
    "> - y_test: dummy, not used, default=None\n",
    "\n",
    "> **Returns**\n",
    "> - results (dict): dictionary including:\n",
    ">    - pred_train (numpy array): predictions for training dataset\n",
    ">    - pred_test (numpy array): predictions for test dataset\n",
    ">    - model (object): trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_RF(X_train, y_train, X_test, y_test=None):\n",
    "    \n",
    "    '''\n",
    "    Model\n",
    "    '''\n",
    "    model = RandomForestRegressor(n_estimators = 125,\n",
    "                                  min_samples_split = 10,\n",
    "                                  min_samples_leaf = 1,\n",
    "                                  max_features = 0.375553860442328,\n",
    "                                  max_depth = 200,\n",
    "                                  bootstrap = True,\n",
    "                                  random_state = SEED,\n",
    "                                  n_jobs = -1)\n",
    "    \n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    time_start = time()\n",
    "    model.fit(X_train, y_train)\n",
    "    time_end = time()\n",
    "    print(f\"Time for training: {time_end - time_start}\")\n",
    "    \n",
    "    '''\n",
    "    Testing\n",
    "    '''\n",
    "    results = {}\n",
    "    results['pred_train'] = model.predict(X_train)\n",
    "    results['pred_test']  = model.predict(X_test)\n",
    "    results['model']      = model\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5540bb8",
   "metadata": {},
   "source": [
    "## eXtreme Gradient Boosting (XGBoost)\n",
    "XGBoost regressor with optimized hyper-parameters for its top mutation matrix GIAG010101\n",
    "\n",
    "> **Parameters**\n",
    "> - X_train (numpy array): input features to train the model\n",
    "> - y_train (numpy array): output labels for supervised learning of the model\n",
    "> - X_test (numpy array): input features to test the model\n",
    "> - y_test: dummy, not used, default=None\n",
    "\n",
    "> **Returns**\n",
    "> - results (dict): dictionary including:\n",
    ">    - pred_train (numpy array): predictions for training dataset\n",
    ">    - pred_test (numpy array): predictions for test dataset\n",
    ">    - model (object): trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e826849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_XGBoost(X_train, y_train, X_test, y_test=None):\n",
    "    \n",
    "    '''\n",
    "    Model\n",
    "    '''\n",
    "    model = XGBRegressor(booster='gbtree',\n",
    "                         n_estimators=343, max_depth=23,\n",
    "                         learning_rate=0.0586498853490469, subsample=0.790391730792872,\n",
    "                         colsample_bytree=0.829414276718852, colsample_bylevel=0.360570017142831,\n",
    "                         n_jobs=-1, random_state = SEED)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    time_start = time()\n",
    "    model.fit(X_train, y_train,\n",
    "              verbose=False)\n",
    "    time_end = time()\n",
    "    print(f\"Time for training: {time_end - time_start}\")\n",
    "    \n",
    "    '''\n",
    "    Testing\n",
    "    '''\n",
    "    results = {}\n",
    "    results['pred_train'] = model.predict(X_train)\n",
    "    results['model']      = model\n",
    "    results['pred_test']  = model.predict(X_test)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138f755f",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron\n",
    "Multi-layer Perceptron with optimized hyperparameters for mutation matrix WEIL970102\n",
    "\n",
    "> **Parameters**\n",
    "> - X_train (numpy array): input features to train the model\n",
    "> - y_train (numpy array): output labels for supervised learning of the model\n",
    "> - X_test (numpy array): input features to test the model\n",
    "> - y_test: dummy, not used, default=None\n",
    "\n",
    "> **Returns**\n",
    "> - results (dict): dictionary including:\n",
    ">    - pred_train (numpy array): predictions for training dataset\n",
    ">    - pred_test (numpy array): predictions for test dataset\n",
    ">    - model (object): trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff7b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_MLP(X_train, y_train, X_test, y_test=None):\n",
    "    \n",
    "    '''\n",
    "    Hyperparameters\n",
    "    '''\n",
    "    learning_rate = 0.0000168309492546526\n",
    "    epochs        = 160\n",
    "    n_layers      = 2\n",
    "    layer_params  = {'n_units_l1': 5000,\n",
    "                     'dropout_l1': 0.4,\n",
    "                     'n_units_l2': 3100,\n",
    "                     'dropout_l2': 0.5}\n",
    "    \n",
    "    '''\n",
    "    Normalization\n",
    "    '''\n",
    "    # Input normalization\n",
    "    normalizer = MinMaxScaler()\n",
    "    X_train    = normalizer.fit_transform(X_train)\n",
    "    X_test     = normalizer.transform(X_test)\n",
    "\n",
    "    # target reshaping\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Model\n",
    "    '''\n",
    "    input1 = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    # hidden layers\n",
    "    for layer in range(1, n_layers+1):\n",
    "        if layer == 1:\n",
    "            # first hidden layer uses input1\n",
    "            x1 = Dense(layer_params[f\"n_units_l{layer}\"])(input1)\n",
    "        else:\n",
    "            x1 = Dense(layer_params[f\"n_units_l{layer}\"])(x1)\n",
    "        x1 = LeakyReLU()(x1)\n",
    "        x1 = Dropout(layer_params[f\"dropout_l{layer}\"])(x1)\n",
    "    \n",
    "    # output layer\n",
    "    x1 = Dense(1)(x1)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input1, outputs = x1)\n",
    "    model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
    "                  optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "                 )\n",
    "    \n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    time_start = time()\n",
    "    model.fit(X_train, y_train,\n",
    "              epochs = epochs,\n",
    "              batch_size = 1024,\n",
    "              shuffle = True,\n",
    "              verbose=0\n",
    "             )\n",
    "    time_end = time()\n",
    "    print(f\"Time for training: {time_end - time_start}\")\n",
    "    \n",
    "    '''\n",
    "    Testing\n",
    "    '''\n",
    "    results = {}\n",
    "    results['pred_train'] = model.predict(X_train, verbose=0).squeeze()\n",
    "    results['pred_test']  = model.predict(X_test, verbose=0).squeeze()\n",
    "    results['model']      = model\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a31492",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "Residual neural network with optimized hyperparameters for mutation matrix MUET010101\n",
    "\n",
    "> **Parameters**\n",
    "> - X_train (numpy array): input features to train the model\n",
    "> - y_train (numpy array): output labels for supervised learning of the model\n",
    "> - X_test (numpy array): input features to test the model\n",
    "> - y_test: dummy, not used, default=None\n",
    "\n",
    "> **Returns**\n",
    "> - results (dict): dictionary including:\n",
    ">    - pred_train (numpy array): predictions for training dataset\n",
    ">    - pred_test (numpy array): predictions for test dataset\n",
    ">    - model (object): trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_ResNet(X_train, y_train, X_test, y_test=None):\n",
    "    \n",
    "    '''\n",
    "    Hyperparameters\n",
    "    '''\n",
    "    lr                = 0.003494896818018\n",
    "    epochs            = 140\n",
    "    n_units_linear    = 3200\n",
    "    n_layers          = 1\n",
    "    layer_params      = {'n_units_rnb_1': 1500,\n",
    "                         'dropout_rnb_1': 0.4,\n",
    "                         'res_dropout_rnb_1': 0}\n",
    "    \n",
    "    '''\n",
    "    Normalization\n",
    "    '''\n",
    "    # Input normalization\n",
    "    normalizer = MinMaxScaler()\n",
    "    X_train    = normalizer.fit_transform(X_train)\n",
    "    X_test     = normalizer.transform(X_test)\n",
    "\n",
    "    # target reshaping\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Model\n",
    "    '''\n",
    "    input1 = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    # initial Linear layer\n",
    "    x1 = Dense(n_units_linear)(input1)\n",
    "    \n",
    "    # ResNetBlock\n",
    "    for resnet_block in range(1, n_layers+1):\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = Dense(layer_params[f\"n_units_rnb_{resnet_block}\"], activation='relu')(x1)\n",
    "        x1 = Dropout(layer_params[f\"dropout_rnb_{resnet_block}\"])(x1)\n",
    "        x1 = Dense(X_train.shape[1])(x1)\n",
    "        x1 = Dropout(layer_params[f\"res_dropout_rnb_{resnet_block}\"])(x1)\n",
    "        x1 = Add()([x1, input1])\n",
    "    \n",
    "    # Prediction block\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = ReLU()(x1)\n",
    "    x1 = Dense(1)(x1)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=input1, outputs = x1)\n",
    "\n",
    "    model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
    "                  optimizer = tf.optimizers.Adam(learning_rate=lr),\n",
    "                  metrics = [tf.metrics.MeanAbsoluteError()])\n",
    "    \n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    time_start = time()\n",
    "    model.fit(X_train, y_train,\n",
    "              epochs = epochs,\n",
    "              batch_size = 1024,\n",
    "              shuffle = True,\n",
    "              verbose=0)\n",
    "    time_end = time()\n",
    "    print(f\"Time for training: {time_end - time_start}\")\n",
    "    \n",
    "    '''\n",
    "    Testing\n",
    "    '''\n",
    "    results = {}\n",
    "    results['pred_train'] = model.predict(X_train, verbose=0).squeeze()\n",
    "    results['pred_test']  = model.predict(X_test, verbose=0).squeeze()\n",
    "    results['model']      = model\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (seasonal_ag_pred_tf)",
   "language": "python",
   "name": "seasonal_ag_pred_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
