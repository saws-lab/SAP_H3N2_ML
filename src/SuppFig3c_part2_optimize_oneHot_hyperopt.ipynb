{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63011fc",
   "metadata": {},
   "source": [
    "# Optimize hyperparameters of AdaBoost for each mutation matrix\n",
    "We will optimize hyperparameters of AdaBoost model for each mutation matrix. The optimization will be based on the MAE performance of the model over four validation seasons from 2012NH to 2013SH."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd57cb7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07094155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from ast import literal_eval\n",
    "import pickle\n",
    "\n",
    "# self defined functions\n",
    "import utilities\n",
    "\n",
    "# for model development\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# for hyperparameter optimization\n",
    "from hyperopt import Trials, tpe, hp, fmin, space_eval\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "# for reproduciblility, fix the randomly generated numbers\n",
    "SEED = 100\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111e570",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "752e19f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid_Seasons = ['2012NH', '2012SH', '2013NH', '2013SH'] # seasons from 2012NH to 2013SH\n",
    "\n",
    "HA1_features  = [f\"HA1_{x}\" for x in range(1,329+1)]\n",
    "meta_features = [\n",
    "                 'virus',   # virus avidity\n",
    "                 'serum',   # antiserum potency\n",
    "                 'virusPassCat',\n",
    "                 'serumPassCat'\n",
    "                 ]   # metadata features\n",
    "\n",
    "metadata   = 'a+p+vPC+sPC'   # label to record which metadata is being used\n",
    "model_name = 'AdaBoost'   # identifier for the type of model to be used\n",
    "mut_mat    = 'one_hot'\n",
    "\n",
    "# 20 valid amino acids\n",
    "aa = ['A','R','N','D','C','Q','E','G','H','I','L','K','M','F','P','S','T','W','Y','V']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c340e81",
   "metadata": {},
   "source": [
    "## Paths and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672fb38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "path_data   = \"../data/\"   # path of data\n",
    "path_result = \"../results/SuppFig3_optimization/\"   # results will be saved in this directory\n",
    "Path(path_result+\"/hyperopt_trials/\").mkdir(parents=True, exist_ok=True)   # make directory if it does not exist already\n",
    "\n",
    "# filenames\n",
    "data_fn     = path_data + \"nhts_ha1_binary.csv\"   # input data\n",
    "optimize_fn = path_result+\"SuppFig3c_optimize_mut_mat_hyperopt.csv\"   # to save optimization results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a3da7",
   "metadata": {},
   "source": [
    "## Read data\n",
    "- Binary encoded genetic difference (seq_diff) (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42854ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aab4a40",
   "metadata": {},
   "source": [
    "## Objective function for hyperopt\n",
    "The objective is to minimize the average MAE over validation seasons. This function will train the model with provided hyperparameters and return the average MAE.\n",
    "\n",
    "> **Parameters**\n",
    "> - params (dict): dictionary of hyperparameters and corresponding values\n",
    "> - data (dataframe): dataset\n",
    "\n",
    "> **Returns**\n",
    "> - avg_mae (float): MAE averaged over validation seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5277cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    actual_all  = []   # to collect measured NHTs across validation seasons\n",
    "    predict_all = []   # to collect predicted NHTs across validation seasons\n",
    "    \n",
    "    # loop through validation seasons\n",
    "    for valid_season in Valid_Seasons:\n",
    "        '''\n",
    "        Train Test Split\n",
    "            - based on seasonal framework\n",
    "            - Train: past virus isolates paired with past sera\n",
    "            - Test: circulating virus isolates paired with past sera\n",
    "        '''\n",
    "        ind_train, ind_valid = utilities.seasonal_trainTestSplit(data[['virus', 'serum', 'virusDate', 'serumDate']],\n",
    "                                                                 valid_season)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Assign training and validation datasets\n",
    "        '''\n",
    "        # training dataset\n",
    "        data_train = data.iloc[ind_train].copy()\n",
    "        data_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # validation dataset\n",
    "        data_valid = data.iloc[ind_valid].copy()\n",
    "        data_valid.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Input features (genetic difference)\n",
    "        '''\n",
    "        # training dataset\n",
    "        # get sequences\n",
    "        X_train_virusSeq = pd.DataFrame(data_train.virusSeq.apply(list).tolist(),\n",
    "                                        index=data_train.index,\n",
    "                                        columns=HA1_features)\n",
    "        X_train_serumSeq = pd.DataFrame(data_train.serumSeq.apply(list).tolist(),\n",
    "                                        index=data_train.index,\n",
    "                                        columns=HA1_features)\n",
    "        \n",
    "        # initialize encoders\n",
    "        # fixed encoding, for each site, encode an amino acid into binary vector of length 20\n",
    "        ohe_virus = OneHotEncoder(categories=[aa] * X_train_virusSeq.shape[1], handle_unknown='ignore')\n",
    "        ohe_serum = OneHotEncoder(categories=[aa] * X_train_serumSeq.shape[1], handle_unknown='ignore')\n",
    "        \n",
    "        # train encoder and transform\n",
    "        X_train_virusSeq = ohe_virus.fit_transform(X_train_virusSeq).toarray()\n",
    "        X_train_serumSeq = ohe_serum.fit_transform(X_train_serumSeq).toarray()\n",
    "        \n",
    "        # element-wise OR operation\n",
    "        X_train = np.logical_or(X_train_virusSeq, X_train_serumSeq) * 1\n",
    "\n",
    "\n",
    "        # validation dataset\n",
    "        # get sequences\n",
    "        X_valid_virusSeq = pd.DataFrame(data_valid.virusSeq.apply(list).tolist(),\n",
    "                                        index=data_valid.index,\n",
    "                                        columns=HA1_features)\n",
    "        X_valid_serumSeq = pd.DataFrame(data_valid.serumSeq.apply(list).tolist(),\n",
    "                                        index=data_valid.index,\n",
    "                                        columns=HA1_features)\n",
    "\n",
    "        # transform\n",
    "        X_valid_virusSeq = ohe_virus.transform(X_valid_virusSeq).toarray()\n",
    "        X_valid_serumSeq = ohe_serum.transform(X_valid_serumSeq).toarray()\n",
    "        \n",
    "        # element-wise OR operation\n",
    "        X_valid = np.logical_or(X_valid_virusSeq, X_valid_serumSeq) * 1\n",
    "\n",
    "        del X_train_virusSeq, X_train_serumSeq, X_valid_virusSeq, X_valid_serumSeq\n",
    "\n",
    "        \n",
    "        '''\n",
    "        Input features (metadata features)\n",
    "        '''\n",
    "        X_train_meta = data_train[meta_features].fillna('None').astype('str')\n",
    "        X_valid_meta = data_valid[meta_features].fillna('None').astype('str')\n",
    "\n",
    "\n",
    "        # one hot encoding\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "        X_train_meta = ohe.fit_transform(X_train_meta).toarray()\n",
    "        X_valid_meta = ohe.transform(X_valid_meta).toarray()\n",
    "\n",
    "        X_train = np.hstack((X_train, X_train_meta))\n",
    "        X_valid = np.hstack((X_valid, X_valid_meta))\n",
    "\n",
    "\n",
    "        del X_train_meta, X_valid_meta\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Training and validation\n",
    "        '''\n",
    "        # to avoid max depth of less than 1\n",
    "        if params['max_depth'] < 1:\n",
    "            params['max_depth'] = 1\n",
    "        \n",
    "        model = AdaBoostRegressor(DecisionTreeRegressor(max_depth=params['max_depth'], max_features=params['max_features']),\n",
    "                                  n_estimators=params['n_estimators'],\n",
    "                                  learning_rate=params['learning_rate'],\n",
    "                                  random_state=SEED)\n",
    "        model.fit(X_train, data_train.nht.values)\n",
    "        predict_valid = model.predict(X_valid)\n",
    "        \n",
    "        '''\n",
    "        save actuals and predictions\n",
    "        '''\n",
    "        actual_all.append(data_valid.nht.values)\n",
    "        predict_all.append(predict_valid)\n",
    "        \n",
    "        ##################\n",
    "        # End seasons loop\n",
    "        ##################\n",
    "    \n",
    "    # combine for averaging\n",
    "    actuals     = np.concatenate(actual_all)\n",
    "    predictions = np.concatenate(predict_all)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    metric or loss (MAE)\n",
    "    '''\n",
    "    avg_mae = mean_absolute_error(actuals, predictions)\n",
    "    \n",
    "    return avg_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188636c",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf5eb91b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████| 50/50 [51:58:27<00:00, 4157.93s/trial, best loss: 0.7527625599483879]\n",
      "{'model': 'AdaBoost', 'metadata': 'a+p+vPC+sPC', 'mut_mat': 'one_hot', 'mae': 0.7527625599483879, 'learning_rate': 1.0760969037630008, 'max_depth': 6490, 'max_features': 0.24438375582247596, 'n_estimators': 780}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    '''\n",
    "    load the trials object\n",
    "    '''\n",
    "    with open(path_result+f\"hyperopt_trials/trials_{mut_mat}.hyperopt\", \"rb\") as f:\n",
    "        trial     = pickle.load(f)\n",
    "        max_evals = len(trial) + 45 \n",
    "except:\n",
    "    # hyperopt initialize trials object\n",
    "    trial = Trials()\n",
    "    max_evals = 45\n",
    "\n",
    "# hyperparameters search space\n",
    "space={'n_estimators': scope.int(hp.quniform('n_estimators', 10, 1000, 10)),\n",
    "       'learning_rate': scope.float(hp.uniform('learning_rate', 0.1, 1.5)),\n",
    "       'max_features': scope.float(hp.uniform('max_features', 0.1, 1)),\n",
    "       'max_depth': scope.int(hp.quniform('max_depth', 1, 10000, 10))\n",
    "      }\n",
    "\n",
    "# hyperopt minimization\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=max_evals, \n",
    "            trials=trial,\n",
    "            rstate=np.random.default_rng(SEED))\n",
    "\n",
    "\n",
    "'''\n",
    "Best hyperparameters\n",
    "'''\n",
    "hyperparams = {'model': model_name,\n",
    "               'metadata': metadata,\n",
    "               'mut_mat': mut_mat,\n",
    "               'mae': trial.best_trial['result']['loss']}\n",
    "\n",
    "best_params = space_eval(space, best)\n",
    "# to avoid max depth less than 1 as we actually used in objective function\n",
    "if best_params['max_depth'] < 1:\n",
    "    best_params['max_depth'] = 1\n",
    "\n",
    "hyperparams.update(best_params)\n",
    "print(hyperparams)\n",
    "\n",
    "'''\n",
    "save results\n",
    "'''\n",
    "utilities.saveDict2CSV([hyperparams], optimize_fn)\n",
    "\n",
    "\n",
    "'''\n",
    "save the trials object\n",
    "'''\n",
    "with open(path_result+f\"hyperopt_trials/trials_{mut_mat}.hyperopt\", \"wb\") as f:\n",
    "    pickle.dump(trial, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b14ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (seasonal_ag_pred_tf)",
   "language": "python",
   "name": "seasonal_ag_pred_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
