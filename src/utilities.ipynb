{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed13706a",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "It contains a number of self defined helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9163c0",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601baeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f943b",
   "metadata": {},
   "source": [
    "# Save dictionary in a CSV file\n",
    "\n",
    "> **Parameters**\n",
    "> - output (dict): dictionary to be saved\n",
    "> - fn (str): CSV filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8129b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDict2CSV(output, fn):\n",
    "\n",
    "    output_df   = pd.DataFrame.from_dict(output) \n",
    "    \n",
    "    # if csv file already exists then append the results in a row\n",
    "    if os.path.isfile(fn):\n",
    "        # save without adding header\n",
    "        output_df.to_csv(fn, index=False, mode='a', header=False)\n",
    "    else:\n",
    "        output_df.to_csv(fn, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90157987",
   "metadata": {},
   "source": [
    "## Data distribution\n",
    "Compute data distribution and save it in a CSV file\n",
    "\n",
    "> **Parameters**\n",
    "> - nhts (array like): nht values\n",
    "> - fn (str): CSV filename to save the data distribution, default=None\n",
    "> - col (list): extra column(s) in a CSV file\n",
    "> - col_val (list): value(s) of extra column(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25144076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_distribution(nhts, fn=None, col=[], col_val=[]):\n",
    "\n",
    "    total         = len(nhts)\n",
    "    variant       = sum(nhts > 2)\n",
    "    similar       = total - variant\n",
    "    variant_ratio = variant/total * 100 if total else 0      # exception for division with zero\n",
    "    \n",
    "    # dictionary for data distribution\n",
    "    dist = {'total':   total,\n",
    "            'variant': variant,\n",
    "            'similar': similar,\n",
    "            'variant_ratio':   variant_ratio}\n",
    "    \n",
    "    if fn:\n",
    "        output = {}\n",
    "\n",
    "        # if 'col' is provided\n",
    "        if len(col) > 0:\n",
    "            # loop through col and save it\n",
    "            for column, column_value in zip(col, col_val):\n",
    "                output[column] = column_value\n",
    "\n",
    "        output.update(dist)\n",
    "\n",
    "        saveDict2CSV([output], fn)\n",
    "    else:\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8511c5d",
   "metadata": {},
   "source": [
    "## Compute scores\n",
    "Compute scores and save in a CSV file\n",
    "    \n",
    "> **Parameters**\n",
    "> - y (numpy array): actual NHTs\n",
    "> - y_pred (numpy array): predicted NHTs\n",
    "> - filename (string): filename to save the computed scores\n",
    "> - thresh (int): threshold value for conversion to labels (default = 2)\n",
    "> - col (list): list denoting extra column(s) in a scores CSV file\n",
    "> - col_val (list): value(s) of extra column(s)\n",
    "> - y_label (numpy array): labels for actual NHTs, (default='')\n",
    "> - y_pred_label (numpy array): labels for predicted NHTs, (default='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(y, y_pred, filename, thresh=2, col=[], col_val=[], y_label='', y_pred_label=''):\n",
    "    \n",
    "    # make sure there is data in the input\n",
    "    if y.size:\n",
    "        # change to 1D array\n",
    "        y      = y.squeeze()\n",
    "        y_pred = y_pred.squeeze()\n",
    "        \n",
    "        # if labels are not provided\n",
    "        if (y_label=='') & (y_pred_label==''):\n",
    "            # compute labels (actual threshold = 2)\n",
    "            y_label      = (y > 2) * 1\n",
    "            y_pred_label = (y_pred > thresh) * 1\n",
    "            \n",
    "    else:\n",
    "        print('Error: empty input, no computation done.')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # scores\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_label, y_pred_label, labels=[0,1]).ravel()\n",
    "    \n",
    "    \n",
    "    # regression metrics\n",
    "    mae = metrics.mean_absolute_error(y, y_pred)\n",
    "    r2  = metrics.r2_score(y, y_pred)\n",
    "    \n",
    "    # classification metrics\n",
    "    acc    = metrics.accuracy_score(y_label, y_pred_label)\n",
    "    recall = metrics.recall_score(y_label, y_pred_label)\n",
    "    spec   = tn/(tn+fp)\n",
    "    prec   = metrics.precision_score(y_label, y_pred_label)\n",
    "    npv    = tn/(tn+fn)\n",
    "    mcc    = metrics.matthews_corrcoef(y_label, y_pred_label)\n",
    "    try:\n",
    "        auroc  = metrics.roc_auc_score(y_label, y_pred)\n",
    "    except ValueError:\n",
    "        auroc  = np.nan\n",
    "    try:\n",
    "        auprc  = metrics.average_precision_score(y_label, y_pred)\n",
    "    except ValueError:\n",
    "        auprc  = np.nan\n",
    "        \n",
    "    \n",
    "    # dictionary of results\n",
    "    scores = {}\n",
    "    \n",
    "    # if col is provided\n",
    "    if len(col) > 0:\n",
    "        # loop through col and save it\n",
    "        for column, column_value in zip(col, col_val):\n",
    "            scores[column] = column_value\n",
    "    \n",
    "    scores['MAE']         = mae\n",
    "    scores['R2']          = r2\n",
    "    scores['Accuracy']    = acc\n",
    "    scores['Sensitivity'] = recall\n",
    "    scores['Specificity'] = spec\n",
    "    scores['MCC']         = mcc\n",
    "    scores['AUROC']       = auroc\n",
    "    scores['Precision']   = prec\n",
    "    scores['NPV']         = npv\n",
    "    scores['AUPRC']       = auprc\n",
    "        \n",
    "    \n",
    "    # save scores to a CSV file\n",
    "    saveDict2CSV([scores], filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b221b4",
   "metadata": {},
   "source": [
    "## Dates as per the influenza season\n",
    "Provide start and end dates of circulating isolates as per the provided season\n",
    "\n",
    "- for NH season: circulating isolates range from \"season_year-1\"-Sep-01 to \"season_year\"-Jan-31\n",
    "- for SH season: circulating isolates range from \"season_year\"-Feb-01 to \"season_year\"-Aug-31\n",
    "\n",
    "> **Parameters**\n",
    "> - season (str): identifier for the Northern of Southern Hemisphere season such as \"2015NH\"\n",
    "\n",
    "> **Returns**\n",
    "> - circ_start (string): start date of circulating isolates as per 'season'.\n",
    "    circ_end (string): end date of circulating isolates as per 'season'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd10ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circulating_dates(season):\n",
    "    \n",
    "    ###########\n",
    "    # NH season\n",
    "    ###########\n",
    "    if season[-2] == \"N\":\n",
    "        # get season year as int\n",
    "        season_year = int(season[:-2])\n",
    "        \n",
    "        # circulating isolates\n",
    "        # time limits according to NH season\n",
    "        circ_start = f\"{season_year-1}-09-01\"    # Sep. start of previous year\n",
    "        circ_end   = f\"{season_year}-01-31\"      # Jan. end\n",
    "    \n",
    "    ###########\n",
    "    # SH season\n",
    "    ###########\n",
    "    elif season[-2] == \"S\":\n",
    "        season_year = int(season[:-2])\n",
    "\n",
    "        # time limits according to SH meeting\n",
    "        circ_start = f\"{season_year}-02-01\"    # Feb. start\n",
    "        circ_end   = f\"{season_year}-08-31\"    # Aug. end\n",
    "\n",
    "   \n",
    "    else:\n",
    "        print(\"Wrong input for season: \", season)\n",
    "        exit(1)\n",
    "    \n",
    "    \n",
    "    return circ_start, circ_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933d6c44",
   "metadata": {},
   "source": [
    "## Seasonal split\n",
    "Split the data into training and test datasets as per influenza seasonsal vaccine composition meetings\n",
    "\n",
    "- Training pairs: past virus isolates paired with past antisera\n",
    "- Test pairs: circulating virus isolates paired with past antisera\n",
    "\n",
    "\n",
    "- for NH season\n",
    "    - past isolates before \"meeting_year-1\"-Sep-01\n",
    "    - circulating isolates between \"meeting_year-1\"-Sep-01 to \"meeting_year\"-Jan-31\n",
    "- for SH meeting\n",
    "    - past isolates before \"meeting_year\"-Feb-01\n",
    "    - circulating isolates between \"meeting_year\"-Feb-01 to \"meeting_year\"-Aug-31\n",
    "\n",
    "\n",
    "> **Parameters**\n",
    "> - data (DataFrame): Whole dataset to be split.\n",
    "> - test_season (string): Identifier for Northern or Southern Hemisphere season such as \"2022NH\" or \"2022SH\".\n",
    "\n",
    "> **Returns**\n",
    "> - train_ind (numpy array): indices of training dataset as per \"test_season\". Dimension = (data.shape[0], 1)\n",
    "> - test_ind (numpy array): indices of test dataset as per \"test_season\". Dimension = (data.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a6f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_trainTestSplit(data, test_season):\n",
    "    # get start and end dates for circulating isolates\n",
    "    circ_start, circ_end = circulating_dates(test_season)\n",
    "    \n",
    "    # indices of circulating virus\n",
    "    ind_circ_virus = data.virusDate.ge(circ_start) & data.virusDate.le(circ_end)\n",
    "    \n",
    "    # indices of past virus and antiserum\n",
    "    ind_past_virus = data.virusDate.lt(circ_start)\n",
    "    ind_past_serum = data.serumDate.lt(circ_start)\n",
    "    \n",
    "    '''\n",
    "    Training dataset\n",
    "    '''\n",
    "    # past virus isolates paired with past antisera\n",
    "    train_ind = ind_past_virus & ind_past_serum\n",
    "    \n",
    "    '''\n",
    "    Test dataset\n",
    "    '''\n",
    "    # circulating virus isolates paired with past antisera\n",
    "    test_ind  = ind_circ_virus & ind_past_serum\n",
    "    \n",
    "    \n",
    "    return train_ind.to_numpy(), test_ind.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910019ff",
   "metadata": {},
   "source": [
    "## Seasonal split, randomly samples fraction of HI titers per season\n",
    "Split the data into training and test datasets as per influenza seasonsal vaccine composition meetings. This function corresponds to the first scenario of robustness tests (Supp. Fig. 5a), which randomly samples a fraction of HI titers in each season from training dataset.\n",
    "\n",
    "> **Parameters**\n",
    "> - data (DataFrame): Whole dataset to be split.\n",
    "> - test_season (str): Either northern or southern hemisphere season identifier such as \"2022NH\" or \"2022SH\"\n",
    "> - titers_train (int): percentage of titers from each season in training dataset (default = 100)\n",
    "> - random_state (int): random seed for random selection of titers_train (default = None)\n",
    "    \n",
    "> **Returns**\n",
    "> - train_ind (numpy array): indices of training dataset as per \"test_season\". Dimension = (data.shape[0], 1)\n",
    "> - test_ind (numpy array): indices of test dataset as per \"test_season\". Dimension = (data.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268548eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rndTitersTrainSeason_seasonal_trainTestSplit(data, test_season, titers_train=100, random_state=None):\n",
    "    \n",
    "    '''\n",
    "    Test dataset\n",
    "    '''\n",
    "    # get start and end dates for circulating isolates\n",
    "    circ_start, circ_end = circulating_dates(test_season)\n",
    "    \n",
    "    # indices of circulating virus\n",
    "    ind_circ_virus = data.virusDate.ge(circ_start) & data.virusDate.le(circ_end)\n",
    "    \n",
    "    # indices of past antisera before current season\n",
    "    ind_past_serum = data.serumDate.lt(circ_start)\n",
    "    \n",
    "    # circulating virus isolates paired with past antisera\n",
    "    test_ind = ind_circ_virus & ind_past_serum\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Training dataset\n",
    "    '''\n",
    "    # if 100% data per meeting in training dataset is required\n",
    "    if titers_train == 100:\n",
    "        # then there is no need to filter data to check robustness\n",
    "        # and we can use all the past data       \n",
    "        \n",
    "        # indices of all the virus isolates before the current meeting\n",
    "        ind_past_virus = data.virusDate.lt(circ_start)\n",
    "        \n",
    "        # training dataset\n",
    "        # past virus isolates paired with past sera\n",
    "        train_ind = ind_past_virus & ind_past_serum\n",
    "        \n",
    "    # if filtering of data per season is required to check robustness\n",
    "    else:\n",
    "        titers_train = titers_train/100     # convertion due to percentage\n",
    "        ind_train = np.empty(0, dtype='object')\n",
    "        \n",
    "        Seasons = [str(year)+s for year in range (2003, 2021) for s in [\"NH\", \"SH\"]]   # seasons from 2003NH to 2020SH\n",
    "                \n",
    "        # for each season, randomly pick data_train percent of data\n",
    "        for season in Seasons:\n",
    "            # if season is before the test season\n",
    "            if season < test_season:\n",
    "                # get start and end dates as per season\n",
    "                season_start, season_end = circulating_dates(season)\n",
    "                \n",
    "                # virus isolates in season\n",
    "                ind_season_virus = data.virusDate.ge(season_start) & data.virusDate.le(season_end)\n",
    "                \n",
    "                # virus-antiserum pairs in season\n",
    "                ind_titers_season = ind_season_virus & ind_past_serum\n",
    "                \n",
    "                # indices of true values in ind_titers_season\n",
    "                index_titers_season = ind_titers_season[ind_titers_season].index\n",
    "                N_titers_season     = len(index_titers_season)\n",
    "                \n",
    "                # random selection\n",
    "                np.random.seed(random_state)\n",
    "                rnd_titers_indices = np.random.choice(N_titers_season, int(titers_train*N_titers_season), replace=False)\n",
    "                \n",
    "                # indices of randomly selected virus-antiserum pairs in a season\n",
    "                ind_train_season = index_titers_season[rnd_titers_indices]\n",
    "                \n",
    "                # store selected virus isolates of this season\n",
    "                ind_train = np.concatenate((ind_train, ind_train_season))\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        # get unique indices for training virus-antiserum pairs\n",
    "        ind_train = list(set(ind_train))\n",
    "        \n",
    "        # indices of virus-antiserum pairs in training dataset\n",
    "        # in the form of boolean series of size equal to dataset\n",
    "        train_ind = pd.Series(False, range(data.shape[0]))\n",
    "        train_ind[ind_train] = True\n",
    "\n",
    "    \n",
    "    return train_ind.to_numpy(), test_ind.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679f8e31",
   "metadata": {},
   "source": [
    "## Seasonal split, randomly samples fraction of isolates per season\n",
    "Split the data into training and test datasets as per influenza seasonsal vaccine composition meetings. This function corresponds to the second scenario of robustness tests (Supp. Fig. 5b), which randomly samples a fraction of virus isolates in each season from training dataset.\n",
    "\n",
    "> **Parameters**\n",
    "> - data (DataFrame): Whole dataset to be split.\n",
    "> - test_season (str): Either northern or southern hemisphere season identifier such as \"2022NH\" or \"2022SH\"\n",
    "> - isolates_train (int): percentage of virus isolates from each season in training dataset (default = 100)\n",
    "> - random_state (int): random seed for random selection of isolates_train (default = None)\n",
    "    \n",
    "> **Returns**\n",
    "> - train_ind (numpy array): indices of training dataset as per \"test_season\". Dimension = (data.shape[0], 1)\n",
    "> - test_ind (numpy array): indices of test dataset as per \"test_season\". Dimension = (data.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3fe4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rndIsolatesTrainSeason_seasonal_trainTestSplit(data, test_season, isolates_train=100, random_state=None):\n",
    "    \n",
    "    '''\n",
    "    Test dataset\n",
    "    '''\n",
    "    # get start and end dates for circulating isolates\n",
    "    circ_start, circ_end = circulating_dates(test_season)\n",
    "    \n",
    "    # indices of circulating virus\n",
    "    ind_circ_virus = data.virusDate.ge(circ_start) & data.virusDate.le(circ_end)\n",
    "    \n",
    "    # indices of past sera before current season\n",
    "    ind_past_serum = data.serumDate.lt(circ_start)\n",
    "    \n",
    "    # test pairs\n",
    "    test_ind = ind_circ_virus & ind_past_serum\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Training dataset\n",
    "    '''\n",
    "    \n",
    "    # if 100% isolates per season in training dataset are required\n",
    "    if isolates_train == 100:\n",
    "        # then there is no need to filter isolates to check robustness\n",
    "        # and we can use all the past virus isolates        \n",
    "        \n",
    "        # indices of all the virus isolates before the current season\n",
    "        ind_past_virus = data.virusDate.lt(circ_start)\n",
    "        \n",
    "        # training dataset\n",
    "        # past virus isolates paired with past antisera\n",
    "        train_ind = ind_past_virus & ind_past_serum\n",
    "        \n",
    "    # if filtering of isolates per season is required to check robustness\n",
    "    else:\n",
    "        isolates_train = isolates_train/100     # convertion due to percentage\n",
    "        train_virus = np.empty(0, dtype='object')\n",
    "        \n",
    "        Seasons = [str(year)+s for year in range (2003, 2021) for s in [\"NH\", \"SH\"]]   # seasons from 2003NH to 2020SH\n",
    "                \n",
    "        # for each season, randomly pick isolates_train percent of isolates\n",
    "        for season in Seasons:\n",
    "            # if season is before the test season\n",
    "            if season < test_season:\n",
    "                # get start and end dates as per season\n",
    "                season_start, season_end = circulating_dates(season)\n",
    "                \n",
    "                # virus isolates in season\n",
    "                ind_season_virus = data.virusDate.ge(season_start) & data.virusDate.le(season_end)\n",
    "                season_virus     = data.virus[ind_season_virus].unique()\n",
    "                N_season_virus   = season_virus.shape[0]\n",
    "                \n",
    "                # random selection\n",
    "                np.random.seed(random_state)\n",
    "                rnd_season_virus = np.random.choice(N_season_virus, int(isolates_train*N_season_virus), replace=False)\n",
    "                season_virus     = season_virus[rnd_season_virus]\n",
    "                \n",
    "                # store selected virus isolates of this season\n",
    "                train_virus = np.concatenate((train_virus, season_virus))\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # indices of selected virus isolates for all season in training dataset\n",
    "        ind_train_virus = np.in1d(data.virus.to_numpy(), train_virus)\n",
    "        \n",
    "        # indices of virus-antiserum pairs in training dataset\n",
    "        # selected virus isolates paired with past sera\n",
    "        train_ind = ind_train_virus & ind_past_serum\n",
    "    \n",
    "    return train_ind.to_numpy(), test_ind.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52e6a7",
   "metadata": {},
   "source": [
    "## Seasonal split, with 1 missed season from training dataset\n",
    "Split the data into training and test datasets as per influenza seasonsal vaccine composition meetings such that the training dataset will consists of all the past data except data from 1 season. This function corresponds to Supp. Fig. 5c.\n",
    "\n",
    "> **Parameters**\n",
    "> - data (DataFrame): Whole dataset to be split.\n",
    "> - test_season (str): Either northern or southern hemisphere season identifier such as \"2022NH\" or \"2022SH\"\n",
    "> - missed_season (str): identifier for the missed season from training dataset\n",
    "    \n",
    "> **Returns**\n",
    "> - train_ind (numpy array): indices of training dataset as per \"test_season\". Dimension = (data.shape[0], 1)\n",
    "> - test_ind (numpy array): indices of test dataset as per \"test_season\". Dimension = (data.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e83e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def miss1TrainSeason_seasonal_trainTestSplit(data, test_season, missed_season):\n",
    "    \n",
    "    '''\n",
    "    Test dataset\n",
    "    '''\n",
    "    # get start and end dates for circulating isolates\n",
    "    circ_start, circ_end = circulating_dates(test_season)\n",
    "    \n",
    "    # indices of circulating virus\n",
    "    ind_circ_virus = data.virusDate.ge(circ_start) & data.virusDate.le(circ_end)\n",
    "    \n",
    "    # indices of past sera before current season\n",
    "    ind_past_serum = data.serumDate.lt(circ_start)\n",
    "    \n",
    "    # test pairs\n",
    "    test_ind = ind_circ_virus & ind_past_serum\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Training dataset\n",
    "    '''\n",
    "    # if there is no missed season for training\n",
    "    if missed_season == 'Reference':\n",
    "        # indices of past virus isolates before current season\n",
    "        ind_past_virus = data.virusDate.lt(circ_start)\n",
    "        \n",
    "        # training dataset\n",
    "        # past virus isolates paired with past antisera\n",
    "        train_ind = ind_past_virus & ind_past_serum\n",
    "    \n",
    "    else:\n",
    "        # get start and end dates of missed season\n",
    "        missed_start, missed_end = circulating_dates(missed_season)\n",
    "        \n",
    "        # indices of virus isolates before missed season and before start of circulating isolates\n",
    "        ind_virus_before_miss = data.virusDate.lt(missed_start) & data.virusDate.lt(circ_start)\n",
    "        # indices of virus isolates after missed season and before start of circulating isolates\n",
    "        ind_virus_after_miss  = data.virusDate.gt(missed_end) & data.virusDate.lt(circ_start)\n",
    "        \n",
    "        # indices of virus isolates for training\n",
    "        ind_train_virus = ind_virus_before_miss | ind_virus_after_miss\n",
    "        \n",
    "        # training dataset\n",
    "        # indices of virus-antiserum pairs in training dataset\n",
    "        train_ind = ind_train_virus & ind_past_serum\n",
    "    \n",
    "    \n",
    "    return train_ind.to_numpy(), test_ind.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f14380",
   "metadata": {},
   "source": [
    "## Seasonal split, training data from recent seasons only\n",
    "Split the data into training and test datasets as per influenza seasonsal vaccine composition meetings, where the training dataset include data from a few recent seasons before the current season.\n",
    "\n",
    "> **Parameters**\n",
    "> - data (DataFrame): Whole dataset to be split.\n",
    "> - test_season (str): Either northern or southern hemisphere season identifier such as \"2022NH\" or \"2022SH\"\n",
    "> - train_seasons ('all' or int): number of recent seasons from training data, (default='all', use all training seasons)\n",
    "    \n",
    "> **Returns**\n",
    "> - train_ind (numpy array): indices of training dataset as per \"test_season\". Dimension = (data.shape[0], 1)\n",
    "> - test_ind (numpy array): indices of test dataset as per \"test_season\". Dimension = (data.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e2283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recentTrainSeasons_seasonal_trainTestSplit(data, test_season, train_seasons='all'):\n",
    "    \n",
    "    '''\n",
    "    Test dataset\n",
    "    '''\n",
    "    # get start and end dates for circulating isolates\n",
    "    circ_start, circ_end = circulating_dates(test_season)\n",
    "    \n",
    "    # indices of circulating virus\n",
    "    ind_circ_virus = data.virusDate.ge(circ_start) & data.virusDate.le(circ_end)\n",
    "    \n",
    "    # indices of past sera before current season\n",
    "    ind_past_serum = data.serumDate.lt(circ_start)\n",
    "    \n",
    "    \n",
    "    # test pairs\n",
    "    test_ind = ind_circ_virus & ind_past_serum\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Training dataset\n",
    "    '''\n",
    "    # identify the past virus start date\n",
    "    ###############\n",
    "    # for NH season\n",
    "    ###############\n",
    "    if test_season[-2] == \"N\":\n",
    "        # get year as int\n",
    "        season_year = int(test_season[:-2])\n",
    "        \n",
    "        # past virus start\n",
    "        # all seasons, H3N2 starts from 1968 for any data source\n",
    "        if train_seasons == 'all':\n",
    "            past_virus_start = \"1968\"\n",
    "        # even number of seasons\n",
    "        # means same NH season\n",
    "        elif train_seasons%2 == 0:\n",
    "            past_virus_start = f\"{season_year-int(train_seasons/2)-1}-09-01\"\n",
    "        # odd number of seasons\n",
    "        # means SH season\n",
    "        elif train_seasons%2 == 1:\n",
    "            past_virus_start = f\"{season_year-int(train_seasons/2)-1}-02-01\"\n",
    "            \n",
    "    ###############\n",
    "    # for SH season\n",
    "    ###############\n",
    "    else:\n",
    "        season_year = int(test_season[:-2])\n",
    "        \n",
    "        # past virus start\n",
    "        # all seasons, H3N2 starts from 1968 for any data source\n",
    "        if train_seasons == 'all':\n",
    "            past_virus_start = \"1968\"\n",
    "        # even number of seasons\n",
    "        # means same SH season\n",
    "        elif train_seasons%2 == 0:\n",
    "            past_virus_start = f\"{season_year-int(train_seasons/2)}-02-01\"\n",
    "        # odd number of seasons\n",
    "        # means NH season\n",
    "        elif train_seasons%2 == 1:\n",
    "            past_virus_start = f\"{season_year-int(train_seasons/2)-1}-09-01\"\n",
    "    \n",
    "    \n",
    "    # indices of past virus isolates from a few recent seasons\n",
    "    ind_past_virus = data.virusDate.ge(past_virus_start) & data.virusDate.lt(circ_start)\n",
    "    \n",
    "    # training pairs\n",
    "    # past virus isolates paired with past sera\n",
    "    train_ind = ind_past_virus & ind_past_serum\n",
    "    \n",
    "    \n",
    "    return train_ind.to_numpy(), test_ind.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07540c",
   "metadata": {},
   "source": [
    "## Seasonal split, circulating isolates in training dataset\n",
    "Split the data into training and test datasets as per influenza seasonsal vaccine composition meetings. Also, include the data corresponding to a few percent of circulating isolates in training dataset. This function corresponds to Supp. Fig. 7.\n",
    "\n",
    "> **Parameters**\n",
    "> - data (DataFrame): Whole dataset to be split.\n",
    "> - test_season (str): Either northern or southern hemisphere season identifier such as \"2022NH\" or \"2022SH\"\n",
    "> - circ_train (int): percentage of circulating virus isolates in training dataset (default=0)\n",
    "> - random_state (int): random seed for random selection of circ_train (default = None)\n",
    "    \n",
    "> **Returns**\n",
    "> - train_ind (numpy array): indices of training dataset as per \"test_season\". Dimension = (data.shape[0], 1)\n",
    "> - test_ind (numpy array): indices of test dataset as per \"test_season\". Dimension = (data.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circIsolatesTrain_seasonal_trainTestSplit(data, test_season, circ_train=0, random_state=None):\n",
    "    \n",
    "    # get start and end dates for circulating isolates\n",
    "    circ_start, circ_end = circulating_dates(test_season)\n",
    "    \n",
    "    # indices of past virus and antiserum\n",
    "    ind_past_virus = data.virusDate.lt(circ_start)\n",
    "    ind_past_serum = data.serumDate.lt(circ_start)\n",
    "    \n",
    "    \n",
    "    # circulating virus\n",
    "    ind_circ_virus = data.virusDate.ge(circ_start) & data.virusDate.le(circ_end)\n",
    "    circ_virus     = data.virus[ind_circ_virus].unique()\n",
    "    N_circ_virus   = circ_virus.shape[0]\n",
    "\n",
    "    \n",
    "    # randomly select 'circ_train' percent of circulating virus isolates\n",
    "    # for training\n",
    "    np.random.seed(random_state)\n",
    "    circ_train = circ_train/100     # convertion due to percentage\n",
    "    \n",
    "    rnd_train_circ_virus = np.random.choice(N_circ_virus, int(circ_train*N_circ_virus), replace=False)\n",
    "    train_circ_virus     = circ_virus[rnd_train_circ_virus]\n",
    "\n",
    "\n",
    "    # indices of train circulating virus isolates\n",
    "    ind_train_circ_virus = np.in1d(data.virus.to_numpy(), train_circ_virus)\n",
    "    \n",
    "    # indices of train circulating serum\n",
    "    ind_train_circ_serum = np.in1d(data.serum.to_numpy(), train_circ_virus)\n",
    "    \n",
    "\n",
    "    # Remaining circulating isolates for testing\n",
    "    test_circ_virus = np.setdiff1d(circ_virus, train_circ_virus)\n",
    "    \n",
    "    # indices of remaining circulating isolates\n",
    "    ind_test_circ_virus = np.in1d(data.virus.to_numpy(), test_circ_virus)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Training dataset\n",
    "    '''\n",
    "    # pairs of past virus isolates paired with past antisera\n",
    "    pairs_upper_left                     = ind_past_virus & ind_past_serum\n",
    "    # pairs of past virus isolates paired with circulating antisera randomly selected for training\n",
    "    pairs_upper_partial_right            = ind_past_virus & ind_train_circ_serum\n",
    "    # pairs of circulating virus isolates selected for training paired with past sera\n",
    "    pairs_train_circ_lower_left          = ind_train_circ_virus & ind_past_serum\n",
    "    # pairs of circulating virus-circulating antisera, both selected for training\n",
    "    pairs_train_circ_lower_partial_right = ind_train_circ_virus & ind_train_circ_serum\n",
    "    \n",
    "    train_ind = pairs_upper_left | pairs_upper_partial_right | pairs_train_circ_lower_left | pairs_train_circ_lower_partial_right\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Test dataset\n",
    "    '''\n",
    "    # pairs of circulating isolates in test dataset paired with past antisera\n",
    "    pairs_test_circ_lower_left          = ind_test_circ_virus & ind_past_serum\n",
    "    # pairs of circulating isolates in test dataset paired with circulating antisera selected for training\n",
    "    pairs_test_circ_lower_partial_right = ind_test_circ_virus & ind_train_circ_serum\n",
    "    \n",
    "    test_ind = pairs_test_circ_lower_left | pairs_test_circ_lower_partial_right\n",
    "    \n",
    "    \n",
    "    return train_ind.to_numpy(), test_ind.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a7b4d",
   "metadata": {},
   "source": [
    "## Threshold optimization using Youden’s J statistic\n",
    "J = Sensitivity + Specificity – 1, or\n",
    "J = Sensitivity + (1 – FalsePositiveRate) – 1, or\n",
    "J = TruePositiveRate – FalsePositiveRate\n",
    "\n",
    "> **Parameters**\n",
    "> - y_label (numpy array): antigenic labels based on NHT values\n",
    "> - pred (numpy array): predicted NHT values by a model\n",
    "> - fig_fn (str): path for threshold curve figure, threshold vs. Youden’s J statistic (YJS)\n",
    "\n",
    "> **Returns**\n",
    "> - threshold (float): threshold value for classifying virus-antiserum pairs based on NHTs and YJS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77897ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def youden_threshold(y_label, pred, fig_fn=None):\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_label, pred)\n",
    "    J = tpr - fpr\n",
    "    \n",
    "    if fig_fn:\n",
    "        fig, ax = plt.subplots(figsize=(5,4))\n",
    "        ax.plot(thresholds, J, 'x-')\n",
    "    \n",
    "        ax.set_xlabel(\"Threshold\")\n",
    "        ax.set_ylabel(\"YJS\")\n",
    "        sns.despine()\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        fig.savefig(fig_fn, format='svg', bbox_inches='tight')\n",
    "    \n",
    "    ix = np.argmax(J)\n",
    "    threshold = thresholds[ix]\n",
    "    \n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e643242",
   "metadata": {},
   "source": [
    "## Change width of seaborn based bars\n",
    "> **Parameters**\n",
    "> - ax: figure axes handle\n",
    "> - new_width (int): width of the bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed49f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_seaborn_width(ax, new_width):\n",
    "    \n",
    "    for patch in ax.patches :\n",
    "        current_width = patch.get_width()\n",
    "        diff = current_width - new_width\n",
    "\n",
    "        # we change the bar width\n",
    "        patch.set_width(new_width)\n",
    "\n",
    "        # we recenter the bar\n",
    "        patch.set_x(patch.get_x() + diff * .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8023c64",
   "metadata": {},
   "source": [
    "## Epitope sites\n",
    "> **Parameters**\n",
    "> - epDef (str): epitope definition of either Bush or Shih, default='Bush'\n",
    "\n",
    "> **Returns**\n",
    "> - ep_sites (dataframe): HA1 site number and corresponding epitope label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ep_sites(epDef='Bush'):\n",
    "    if epDef == 'Shih' :\n",
    "        '''\n",
    "        Epitope sites defined by Shih et. al. (2007)\n",
    "        Total 62 = A-12, B-16, C-8, D-13, E-13\n",
    "        '''\n",
    "        epA = [122,124,126,131,133,135,137,142,143,144,145,146]\n",
    "        epB = [155,156,157,158,159,160,163,164,186,188,189,190,192,193,196,197]\n",
    "        epC = [50,53,54,275,276,278,299,307]\n",
    "        epD = [121,172,173,174,201,207,213,217,226,227,242,244,248]\n",
    "        epE = [57,62,63,67,75,78,81,82,83,92,94,260,262]\n",
    "        \n",
    "    \n",
    "    elif epDef == 'Bush':\n",
    "        '''\n",
    "        Epitope sites defined by Bush et. al. (1999)\n",
    "        Total 131 = A-19, B-22, C-27, D-41, E-22\n",
    "        '''\n",
    "        epA = [122,124,126,130,131,132,133,135,137,138,140,142,143,144,145,146,150,152,168]\n",
    "        epB = [128,129,155,156,157,158,159,160,163,164,165,186,187,188,189,190,\n",
    "               192,193,194,196,197,198]\n",
    "        epC = [44,45,46,47,48,50,51,53,54,273,275,276,278,279,280,294,297,299,\n",
    "               300,304,305,307,308,309,310,311,312]\n",
    "        epD = [96,102,103,117,121,167,170,171,172,173,174,175,176,177,179,182,\n",
    "               201,203,207,208,209,212,213,214,215,216,217,218,219,226,227,228,\n",
    "               229,230,238,240,242,244,246,247,248]\n",
    "        epE = [57,59,62,63,67,75,78,80,81,82,83,86,87,88,91,92,94,109,260,261,262,265]\n",
    "\n",
    "    \n",
    "    ep_sites = pd.DataFrame(index=[epA+epB+epC+epD+epE])\n",
    "    ep_sites.loc[epA, \"epitope\"] = \"A\"\n",
    "    ep_sites.loc[epB, \"epitope\"] = \"B\"\n",
    "    ep_sites.loc[epC, \"epitope\"] = \"C\"\n",
    "    ep_sites.loc[epD, \"epitope\"] = \"D\"\n",
    "    ep_sites.loc[epE, \"epitope\"] = \"E\"\n",
    "    ep_sites.reset_index(inplace=True)\n",
    "    ep_sites.rename(columns={\"level_0\":\"site\"}, inplace=True)\n",
    "    ep_sites[\"site\"] = \"HA1_\" + ep_sites.site.astype(\"str\")\n",
    "    \n",
    "    return ep_sites"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:seasonal_ag_pred]",
   "language": "python",
   "name": "conda-env-seasonal_ag_pred-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
