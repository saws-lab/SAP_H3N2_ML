{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63011fc",
   "metadata": {},
   "source": [
    "# Optimize hyperparameters of MLP for each mutation matrix\n",
    "We will optimize hyperparameters of the Multilayer Perceptron (MLP) model for each mutation matrix. The optimization will be based on the MAE performance of model over four validation seasons from 2012NH to 2013SH."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd57cb7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07094155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from ast import literal_eval\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# self defined functions\n",
    "import utilities\n",
    "\n",
    "# for model development\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, LeakyReLU\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# for hyperparameter optimization\n",
    "import optuna\n",
    "\n",
    "# for reproduciblility, fix the randomly generated numbers\n",
    "SEED = 100\n",
    "tf.keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111e570",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "752e19f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid_Seasons = ['2012NH', '2012SH', '2013NH', '2013SH'] # seasons from 2012NH to 2013SH\n",
    "\n",
    "HA1_features  = [f\"HA1_{x}\" for x in range(1,329+1)]\n",
    "meta_features = [\n",
    "                 'virus',   # virus avidity\n",
    "                 'serum',   # antiserum potency\n",
    "                 'virusPassCat',\n",
    "                 'serumPassCat'\n",
    "                 ]   # metadata features\n",
    "\n",
    "metadata   = 'a+p+vPC+sPC'   # label to record which metadata is being used\n",
    "model_name = 'MLP'   # identifier for the type of model to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c340e81",
   "metadata": {},
   "source": [
    "## Paths and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672fb38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "path_data   = \"../data/\"   # path of data\n",
    "path_result = \"../results/SuppFig6_comparison/\"   # results will be saved in this directory\n",
    "Path(path_result+f\"/optuna_{model_name}/\").mkdir(parents=True, exist_ok=True)   # make directory if it does not exist already\n",
    "\n",
    "# filenames\n",
    "mut_mat_fn  = path_data + \"aaIndID_selected.txt\"   # filename of list of valid mutation matrics\n",
    "optimize_fn = path_result+f\"SuppFig6_optimize_{model_name}_mut_mat_optuna.csv\"   # to save optimization results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a65cab",
   "metadata": {},
   "source": [
    "## Read valid mutation matrices used for encoding genetic difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c29600f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_mat_List = ['RUSR970101']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a3da7",
   "metadata": {},
   "source": [
    "## Indices of training and validation datasets for validation seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b42854ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset temporarily\n",
    "dummy = pd.read_csv(path_data+\"nhts_ha1_binary.csv\",\n",
    "                    converters={\"seq_diff\": literal_eval})\n",
    "\n",
    "# to collect train and valid indices for each validation season\n",
    "indices_folds = []\n",
    "\n",
    "# loop through each validation season\n",
    "for valid_season in Valid_Seasons:\n",
    "    '''\n",
    "    Train Test Split\n",
    "        - based on seasonal framework\n",
    "        - Train: past virus isolates paired with past sera\n",
    "        - Test: circulating virus isolates paired with past sera\n",
    "    '''\n",
    "    ind_train, ind_valid = utilities.seasonal_trainTestSplit(dummy.copy(), valid_season)\n",
    "    \n",
    "    indices_folds.append((ind_train, ind_valid))\n",
    "\n",
    "del dummy, ind_train, ind_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad0b5b4",
   "metadata": {},
   "source": [
    "## Objective function for optuna\n",
    "The objective is to minimize the average MAE over validation seasons. This function will train the RF model with provided hyperparameters and return the average MAE.\n",
    "\n",
    "> **Parameters**\n",
    "> - trial: object of optuna\n",
    "\n",
    "> **Returns**\n",
    "> - avg_mae (float): MAE averaged over validation seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab0a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    actual_all  = []   # to collect measured NHTs across validation seasons\n",
    "    predict_all = []   # to collect predicted NHTs across validation seasons\n",
    "    \n",
    "    # loop through validation seasons\n",
    "    for ind_train, ind_valid in indices_folds:\n",
    "        '''\n",
    "        Assign training and validation datasets\n",
    "        '''\n",
    "        # training dataset\n",
    "        data_train = data.iloc[ind_train].copy()\n",
    "        data_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # validation dataset\n",
    "        data_valid = data.iloc[ind_valid].copy()\n",
    "        data_valid.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Input features (genetic difference)\n",
    "        '''\n",
    "        # training dataset\n",
    "        X_train = pd.DataFrame(data_train.seq_diff.to_list(),\n",
    "                               index=data_train.index,\n",
    "                               columns=HA1_features)\n",
    "        X_train.fillna(0, inplace=True)   # replace nan with 0\n",
    "\n",
    "        # validation dataset\n",
    "        X_valid = pd.DataFrame(data_valid.seq_diff.to_list(),\n",
    "                               index=data_valid.index,\n",
    "                               columns=HA1_features)\n",
    "        X_valid.fillna(0, inplace=True)   # replace nan with 0\n",
    "\n",
    "\n",
    "        '''\n",
    "        Input features (metadata features)\n",
    "        '''\n",
    "        X_train_meta = data_train[meta_features].fillna('None').astype('str')\n",
    "        X_valid_meta = data_valid[meta_features].fillna('None').astype('str')\n",
    "\n",
    "\n",
    "        # one hot encoding\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "        X_train_meta = ohe.fit_transform(X_train_meta).toarray()\n",
    "        X_valid_meta = ohe.transform(X_valid_meta).toarray()\n",
    "\n",
    "        X_train = np.hstack((X_train.values, X_train_meta))\n",
    "        X_valid = np.hstack((X_valid.values, X_valid_meta))\n",
    "\n",
    "\n",
    "        del X_train_meta, X_valid_meta\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Scaling\n",
    "        '''\n",
    "        # Input normalization\n",
    "        normalizer = MinMaxScaler()\n",
    "        X_train    = normalizer.fit_transform(X_train)\n",
    "        X_valid    = normalizer.transform(X_valid)\n",
    "        \n",
    "        # target reshaping\n",
    "        y_train = data_train.nht.values.reshape(-1, 1)\n",
    "        y_valid = data_valid.nht.values.reshape(-1, 1)\n",
    "        \n",
    "        del data_train, data_valid\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Model\n",
    "        '''\n",
    "        # hyperparameters for optimization\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "        epochs = trial.suggest_int(\"epochs\", 10, 200, step=10)\n",
    "        \n",
    "        # model\n",
    "        input1 = Input(shape=(X_train.shape[1],))\n",
    "            \n",
    "        # hidden layers\n",
    "        n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "        \n",
    "        for layer in range(1, n_layers+1):\n",
    "            n_units = trial.suggest_int(f\"n_units_l{layer}\", 100, 5000, step=100)   # search variable\n",
    "            dropout = trial.suggest_float(f\"dropout_l{layer}\", 0.0, 0.5, step=0.1)   # search variable\n",
    "            \n",
    "            if layer == 1:\n",
    "                # first hidden layer uses input1\n",
    "                x1 = Dense(n_units)(input1)\n",
    "            else:\n",
    "                x1 = Dense(n_units)(x1)\n",
    "            x1 = LeakyReLU()(x1)\n",
    "            x1 = Dropout(dropout)(x1)\n",
    "        \n",
    "        # output layer\n",
    "        x1 = Dense(1)(x1)\n",
    "\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input1, outputs = x1)\n",
    "        model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
    "                      optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "                     )\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Training and validation\n",
    "        '''\n",
    "        model.fit(X_train, y_train,\n",
    "                  epochs = epochs,\n",
    "                  batch_size = 1024,\n",
    "                  shuffle = True,\n",
    "                  verbose=0\n",
    "                 )\n",
    "        predict_valid = model.predict(X_valid, verbose=0).squeeze()\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        save actuals and predictions\n",
    "        '''\n",
    "        actual_all.append(y_valid.squeeze())\n",
    "        predict_all.append(predict_valid)\n",
    "        \n",
    "        ##################\n",
    "        # End seasons loop\n",
    "        ##################\n",
    "    \n",
    "    actuals     = np.concatenate(actual_all)\n",
    "    predictions = np.concatenate(predict_all)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    metric or loss (MAE)\n",
    "    '''\n",
    "    avg_mae = mean_absolute_error(actuals, predictions)\n",
    "    \n",
    "    return avg_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188636c",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5eb91b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutation matrix:  RUSR970101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-13 03:58:46,063] A new study created in memory with name: no-name-f79e0369-b6e7-4287-ae85-aafac12e4771\n",
      "[I 2023-10-13 04:02:15,776] Trial 0 finished with value: 1.0251533836819375 and parameters: {'learning_rate': 0.0003596278592172085, 'epochs': 10, 'n_layers': 2, 'n_units_l1': 4200, 'dropout_l1': 0.4, 'n_units_l2': 4000, 'dropout_l2': 0.5}. Best is trial 0 with value: 1.0251533836819375.\n",
      "[I 2023-10-13 04:12:07,865] Trial 1 finished with value: 0.9728140740767982 and parameters: {'learning_rate': 0.0029831942429423097, 'epochs': 110, 'n_layers': 1, 'n_units_l1': 4900, 'dropout_l1': 0.30000000000000004}. Best is trial 1 with value: 0.9728140740767982.\n",
      "[I 2023-10-13 04:27:49,210] Trial 2 finished with value: 0.9284832231609706 and parameters: {'learning_rate': 0.0003024328698422773, 'epochs': 20, 'n_layers': 5, 'n_units_l1': 3300, 'dropout_l1': 0.0, 'n_units_l2': 5000, 'dropout_l2': 0.30000000000000004, 'n_units_l3': 2300, 'dropout_l3': 0.5, 'n_units_l4': 3300, 'dropout_l4': 0.1, 'n_units_l5': 1900, 'dropout_l5': 0.30000000000000004}. Best is trial 2 with value: 0.9284832231609706.\n",
      "[I 2023-10-13 04:33:04,133] Trial 3 finished with value: 1.0320825668830707 and parameters: {'learning_rate': 0.00012136065533892456, 'epochs': 30, 'n_layers': 4, 'n_units_l1': 700, 'dropout_l1': 0.30000000000000004, 'n_units_l2': 2800, 'dropout_l2': 0.4, 'n_units_l3': 1800, 'dropout_l3': 0.5, 'n_units_l4': 2200, 'dropout_l4': 0.2}. Best is trial 2 with value: 0.9284832231609706.\n",
      "[I 2023-10-13 04:36:05,571] Trial 4 finished with value: 1.273583638611937 and parameters: {'learning_rate': 0.0037600393149155513, 'epochs': 20, 'n_layers': 3, 'n_units_l1': 1700, 'dropout_l1': 0.1, 'n_units_l2': 2400, 'dropout_l2': 0.0, 'n_units_l3': 2200, 'dropout_l3': 0.5}. Best is trial 2 with value: 0.9284832231609706.\n",
      "[I 2023-10-13 04:46:18,952] Trial 5 finished with value: 1.0244941791777926 and parameters: {'learning_rate': 0.00030389714887502973, 'epochs': 130, 'n_layers': 1, 'n_units_l1': 4800, 'dropout_l1': 0.5}. Best is trial 2 with value: 0.9284832231609706.\n",
      "[I 2023-10-13 04:51:39,679] Trial 6 finished with value: 1.0384332463831212 and parameters: {'learning_rate': 0.013887183829077688, 'epochs': 140, 'n_layers': 2, 'n_units_l1': 100, 'dropout_l1': 0.4, 'n_units_l2': 4400, 'dropout_l2': 0.30000000000000004}. Best is trial 2 with value: 0.9284832231609706.\n",
      "[I 2023-10-13 05:51:39,953] Trial 7 finished with value: 0.9328054382418334 and parameters: {'learning_rate': 0.00032435079868919, 'epochs': 110, 'n_layers': 5, 'n_units_l1': 4700, 'dropout_l1': 0.0, 'n_units_l2': 3600, 'dropout_l2': 0.1, 'n_units_l3': 3300, 'dropout_l3': 0.30000000000000004, 'n_units_l4': 3100, 'dropout_l4': 0.0, 'n_units_l5': 600, 'dropout_l5': 0.30000000000000004}. Best is trial 2 with value: 0.9284832231609706.\n",
      "[I 2023-10-13 06:47:16,517] Trial 8 finished with value: 1.018796967632717 and parameters: {'learning_rate': 0.0007936384877547995, 'epochs': 170, 'n_layers': 5, 'n_units_l1': 3200, 'dropout_l1': 0.30000000000000004, 'n_units_l2': 1200, 'dropout_l2': 0.0, 'n_units_l3': 1800, 'dropout_l3': 0.5, 'n_units_l4': 4600, 'dropout_l4': 0.1, 'n_units_l5': 1500, 'dropout_l5': 0.1}. Best is trial 2 with value: 0.9284832231609706.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "loop through mutation matrices\n",
    "'''\n",
    "for mut_mat in mut_mat_List:\n",
    "    \n",
    "    print(\"Mutation matrix: \", mut_mat)\n",
    "    \n",
    "    '''\n",
    "    Dataset\n",
    "    '''\n",
    "    # Genetic difference (seq_diff) encoded as per the mutation matrix\n",
    "    # Converter is used to load the genetic difference saved as a list of floats\n",
    "    data = pd.read_csv(path_data+f\"nhts_ha1_{mut_mat}.csv\",\n",
    "                       converters={\"seq_diff\": literal_eval})\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Hyper-parameter optimization\n",
    "    '''\n",
    "    try:\n",
    "        '''\n",
    "        load the optuna study object\n",
    "        '''\n",
    "        with open(path_result+f\"optuna_{model_name}/study_{mut_mat}.optuna\", \"rb\") as f:\n",
    "            study     = pickle.load(f)\n",
    "            n_trials  = 5\n",
    "    except:\n",
    "        # hyperopt initialize trials object\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        n_trials = 50\n",
    "    \n",
    "    # optuna minimization\n",
    "    study.optimize(objective, n_trials=n_trials, gc_after_trial=True)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Best hyperparameters\n",
    "    '''\n",
    "    hyperparams = {'model': model_name,\n",
    "                   'metadata': metadata,\n",
    "                   'mut_mat': mut_mat,\n",
    "                   'mae': study.best_value\n",
    "                  }\n",
    "    hyperparams.update(study.best_params)\n",
    "    print(hyperparams)\n",
    "    \n",
    "    # save to CSV\n",
    "    utilities.saveDict2CSV([hyperparams], optimize_fn)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    save the trials object\n",
    "    '''\n",
    "    with open(path_result+f\"optuna_{model_name}/study_{mut_mat}.optuna\", \"wb\") as f:\n",
    "        pickle.dump(study, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608edde1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:seasonal_ag_pred_tf]",
   "language": "python",
   "name": "conda-env-seasonal_ag_pred_tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
